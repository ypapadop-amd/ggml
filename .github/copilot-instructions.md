# Copilot Instructions for ggml

## Project Overview

ggml is a tensor library for machine learning with:
- Low-level cross-platform implementation
- Integer quantization support
- Broad hardware support (CPU, CUDA, HIP, SYCL, Metal, OpenCL, Vulkan, WebGPU)
- Automatic differentiation
- No third-party dependencies
- Zero memory allocations during runtime

**Note**: This project is under active development. Some development happens in the [llama.cpp](https://github.com/ggerganov/llama.cpp) and [whisper.cpp](https://github.com/ggerganov/whisper.cpp) repositories.

## Build System

- **Build Tool**: CMake (minimum version 3.14)
- **Languages**: C (C11), C++ (C++17), CUDA, Python
- **Primary Build Command**:
  ```bash
  mkdir build && cd build
  cmake ..
  cmake --build . --config Release -j 8
  ```

## Code Organization

- `src/` - Core ggml library implementation
  - `ggml-cuda/` - CUDA backend
  - `ggml-hip/` - HIP/ROCm backend
  - `ggml-sycl/` - SYCL backend
  - `ggml-metal/` - Metal backend
  - `ggml-opencl/` - OpenCL backend
  - `ggml-vulkan/` - Vulkan backend
  - Other backend implementations
- `include/` - Public headers
- `examples/` - Example programs and utilities
- `tests/` - Test suite
- `scripts/` - Build and sync scripts
- `ci/` - CI/CD scripts

## Testing

- Tests are located in `tests/`
- Run tests with: `ctest --output-on-failure`
- Some tests may be excluded: `ctest --output-on-failure -E "test-opt|test-backend-ops"`

## Hardware Backend Support

When making changes, consider the following backends:
- **CUDA**: `-DGGML_CUDA=ON`
- **HIP**: `-DGGML_HIP=ON`
- **SYCL**: `-DGGML_SYCL=ON`
- **Metal**: `-DGGML_METAL=ON`
- **OpenCL**: `-DGGML_OPENCL=ON`
- **Vulkan**: `-DGGML_VULKAN=ON`

## Coding Guidelines

1. **Standards**:
   - C code: C11 standard
   - C++ code: C++17 standard
   - Follow existing code style in the repository

2. **Changes to Core Library**:
   - For changes to the core `ggml` library or CMake build system, consider opening a PR in https://github.com/ggml-org/llama.cpp first
   - This ensures better visibility, testing, and review

3. **No Third-Party Dependencies**:
   - ggml has no third-party dependencies by design
   - Do not add new dependencies without strong justification

4. **Memory Management**:
   - Zero memory allocations during runtime is a key design principle
   - Be mindful of memory allocation patterns

## Python Dependencies

Python is used for some utilities. Install dependencies:
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Synchronization Scripts

- `scripts/sync-llama.sh` and `scripts/sync-llama-am.sh` - Sync with llama.cpp
- `scripts/sync-whisper.sh` and `scripts/sync-whisper-am.sh` - Sync with whisper.cpp
- These scripts are used to keep ggml in sync with downstream projects

## Contribution Guidelines

Follow [llama.cpp's contribution guidelines](https://github.com/ggml-org/llama.cpp/blob/master/CONTRIBUTING.md) for this project.

## File Generation

Some files are auto-generated:
- `src/ggml-cuda/template-instances/*.cu` - Generated by `generate_cu_files.py`
- `AUTHORS` - Generated by `scripts/gen-authors.sh`
- Do not manually edit these files
